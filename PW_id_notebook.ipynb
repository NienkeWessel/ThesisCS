{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f4e667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 08:13:15.592720: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-24 08:13:15.720130: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-07-24 08:13:15.720149: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-07-24 08:13:16.383530: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-24 08:13:16.383595: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-24 08:13:16.383602: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import time\n",
    "\n",
    "from Levenshtein import distance as lev\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d4c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66417da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea4516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67afa464",
   "metadata": {},
   "source": [
    "# Read (pass)words from file\n",
    "Initially we only read the first 100 passwords\n",
    "\n",
    "If you want to read all the lines, use .readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7746d178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1584b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_passwords(filename, comp_nr_lines, nr_lines):\n",
    "    passwords = []\n",
    "    comparison_pw = []\n",
    "\n",
    "    with open(filename, \"r\") as file:\n",
    "        for i in range(comp_nr_lines):\n",
    "            comparison_pw.append(next(file).strip())\n",
    "        for i in range(nr_lines):\n",
    "            passwords.append(next(file).strip())\n",
    "            \n",
    "    return passwords, comparison_pw\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9512c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words(filename, nr_lines):\n",
    "    words = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        for i in range(nr_lines):\n",
    "            words.append(next(file).strip().split(\"\\t\")[1])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa44a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_labels(passwords, words):\n",
    "    return np.concatenate((np.ones(len(passwords)), np.zeros(len(words))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10918d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_labels(passwords, words):\n",
    "    return torch.hstack((torch.concatenate((torch.ones((1,len(passwords))), torch.zeros(1,len(words))), axis=0),\n",
    "                         torch.concatenate((torch.zeros((1,len(passwords))), torch.ones((1,len(words)))), axis=0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61959c3d",
   "metadata": {},
   "source": [
    "Passwords are 1 and nonpassword strings are 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41976a7a",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "Build features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28699350",
   "metadata": {},
   "source": [
    "## Levenshtein\n",
    "To speed up this calculation, it might be worth it to create an implementation of Levenshtein that stops as soon as a distance larger than the current lowest is found. This lowers the average time complexity (but not worst case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "584c6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_levenshtein_distance(word, passwords):\n",
    "    low = 42000\n",
    "    for pw in passwords:\n",
    "        d = lev(word, pw, score_cutoff=low-1)\n",
    "        if d < low:\n",
    "            low = d\n",
    "        if low == 0:\n",
    "            return low\n",
    "    return low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b1b41",
   "metadata": {},
   "source": [
    "## Other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e83f5e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_non_repeating(text):\n",
    "    '''\n",
    "    Remove repeating letters from a string\n",
    "    E.g. aaabbbccccccaaa becomes abca\n",
    "    \n",
    "    text: input text\n",
    "    return: text without repeating letters\n",
    "    '''\n",
    "    count = 0\n",
    "    for i, c in enumerate(text):\n",
    "        if i ==0 or c != text[i-1]:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "def counts(word, levenshtein=False):\n",
    "    alpha_lower = 0\n",
    "    alpha_upper = 0\n",
    "    numeric = 0\n",
    "    special = 0\n",
    "    s = \"\"\n",
    "    for c in word:\n",
    "        if c.islower():\n",
    "            alpha_lower += 1\n",
    "            s += 'L'\n",
    "        elif c.isupper():\n",
    "            alpha_upper += 1\n",
    "            s += 'U'\n",
    "        elif c.isnumeric():\n",
    "            numeric += 1\n",
    "            s += 'N'\n",
    "        else:\n",
    "            special += 1\n",
    "            s += 'S'\n",
    "    length = len(word)\n",
    "    char_sets = bool(alpha_lower) + bool(alpha_upper) + bool(numeric) + bool(special)\n",
    "    if levenshtein: \n",
    "        lev_d = calculate_levenshtein_distance(word, comparison_pw)\n",
    "        return [length, alpha_lower, alpha_lower/length, alpha_upper, alpha_upper/length, numeric, numeric/length, special, special/length, char_sets, count_non_repeating(s), lev_d]\n",
    "    else:\n",
    "        return [length, alpha_lower, alpha_lower/length, alpha_upper, alpha_upper/length, numeric, numeric/length, special, special/length, char_sets, count_non_repeating(s)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc284ea",
   "metadata": {},
   "source": [
    "## ngram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8311d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def word2ngrams(word, n=2):\n",
    "#    return [' ' + word[:n-1]] + [\"\".join(j) for j in zip(*[word[i:] for i in range(n)])] + [word[-(n-1):] + ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "861103c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(word2ngrams(\"test\", 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed7185",
   "metadata": {},
   "source": [
    "# Machine Learning shizzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fde0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "332badc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_errors(y_test, y_pred):\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] != y_pred[i]:\n",
    "            print(words_test[i])\n",
    "            print(y_test[i])\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c259174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_confusion_matrix(matrices):\n",
    "    cumul_matrices = np.zeros(matrices[0].shape)\n",
    "    for matrix in matrices:\n",
    "        cumul_matrices = np.add(cumul_matrices, matrix)\n",
    "    return cumul_matrices / len(matrices)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51433a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ML_model(total, labels, nr_splits = 10, model = \"decisiontree\"):\n",
    "    \n",
    "    accuracies = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    kf = KFold(n_splits = nr_splits, shuffle=True, random_state = 42)\n",
    "    for train, test in kf.split(total):\n",
    "        \n",
    "        X_train_tot, X_test_tot, y_train, y_test = total[train], total[test], labels[train], labels[test]\n",
    "        X_train = [features for _, features in X_train_tot]\n",
    "        words_train = [words for words, _ in X_train_tot]\n",
    "\n",
    "        X_test = [features for _, features in X_test_tot]\n",
    "        words_test = [words for words, _ in X_test_tot]\n",
    "\n",
    "\n",
    "        # pick model\n",
    "        if model == \"decisiontree\":\n",
    "            clf = tree.DecisionTreeClassifier()\n",
    "        elif model == \"randomforest\":\n",
    "            clf = RandomForestClassifier(max_depth = 10, random_state=0)\n",
    "        elif model == \"GaussianNB\":\n",
    "            clf = GaussianNB()\n",
    "        elif model == \"MultinomialNB\":\n",
    "            clf = MultinomialNB()\n",
    "        else:\n",
    "            raise ValueError('Unknown model type supplied: \"{}\". Please specify a different model type'.format(model))\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_pred, y_test))\n",
    "        confusion_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    print(\"Accuracy score: {}\".format(np.mean(accuracies)))\n",
    "\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(average_confusion_matrix(confusion_matrices))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aeb81e",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "447d45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f10de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "lr = 0.01\n",
    "n_folds = 5\n",
    "lstm_input_size = 32\n",
    "hidden_state_size = 256\n",
    "n_layers = 2\n",
    "dropout = 0.125\n",
    "bidirectional = True\n",
    "batch_size = 30\n",
    "num_sequence_layers = 2\n",
    "output_dim = 11                       # !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "num_time_steps = 4000                 # !!!!!!!!!!!!!!\n",
    "rnn_type = 'LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d5e2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=11, num_layers=2, rnn_type='LSTM'):\n",
    "        super(Bi_RNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #Define the initial linear hidden layer\n",
    "        self.init_linear = nn.Linear(self.input_dim, self.input_dim)\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = eval('nn.' + rnn_type)(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear1 = nn.Linear(512,128)\n",
    "        self.linear2 = nn.Linear(128,8)\n",
    "        self.linear3 = nn.Linear(8,2)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Creating PackedSequence\n",
    "        #packing = nn.utils.rnn.pad_sequence(input)\n",
    "        #                                                  !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # Insert packing stuff\n",
    "        \n",
    "        \n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [batch_size, input_size ,hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both\n",
    "        # have shape (batch_size, num_layers, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(linear_input)\n",
    "        \n",
    "        \n",
    "        # Unpacking PackedSequence\n",
    "        #                                                  !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # Insert packing stuff\n",
    "        \n",
    "        \n",
    "        # Something with Permute???????                    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        \n",
    "        # Tanh\n",
    "        nn.Tanh(input)\n",
    "        \n",
    "        # MaxPool 1D\n",
    "        nn.MaxPool1d()\n",
    "        \n",
    "        # Tanh\n",
    "        nn.Tanh(input)\n",
    "        \n",
    "        # Squeeze\n",
    "        \n",
    "        # First linear layer\n",
    "        input = self.linear1(input)\n",
    "        \n",
    "        # Dropout\n",
    "        \n",
    "        # Second linear layer\n",
    "        input = self.linear2(input)\n",
    "        \n",
    "        # Dropout\n",
    "        \n",
    "        # Third linear layer\n",
    "        output = self.linear3(input)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1224e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Sequential(Dataset):\n",
    "    def __init__(self, input, output):\n",
    "        self.embedding = nn.Embedding(100,32)\n",
    "        self.input = self.embedding(input)\n",
    "        self.output = output\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input[idx]\n",
    "        y = self.output[idx]\n",
    "        # Embed the input in numbers rather than characters\n",
    "        x = self.embedding(x)\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "        return x, y\n",
    "\n",
    "class Dataset_Sequential_test(Dataset):\n",
    "    def __init__(self, input):\n",
    "        self.embedding = nn.Embedding(100,32)\n",
    "        self.input = self.embedding(input)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input[idx]\n",
    "        # Embed the input in numbers rather than characters\n",
    "        x = self.embedding(x)\n",
    "        x = torch.tensor(x, dtype=torch.str)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a8c86c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    test = Dataset_Sequential_test(X_test)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb06c602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.4172e+00, -7.8747e-01, -1.1008e+00,  4.1487e-01,  1.7343e+00],\n",
      "         [ 1.7072e+00, -2.6369e-01, -1.0030e+00, -3.0641e-01, -1.0493e+00],\n",
      "         [-7.6069e-01,  2.5529e+00, -3.9879e-01,  5.7225e-01, -2.6085e-01],\n",
      "         [-4.3744e-01,  3.9818e-01, -8.8142e-01, -6.7287e-02, -7.2837e-01]],\n",
      "\n",
      "        [[-7.4815e-01, -9.4549e-01, -2.4928e-01,  3.3547e-01, -8.6195e-01],\n",
      "         [-8.9873e-01, -8.4740e-01,  1.0274e+00,  1.3147e+00, -1.0529e+00],\n",
      "         [ 1.2640e-01,  1.2569e-01, -6.1100e-01,  1.6612e+00, -1.8537e-01],\n",
      "         [ 2.6297e+00, -1.0318e+00,  4.5603e-02, -7.7397e-01,  5.0445e-01]],\n",
      "\n",
      "        [[-9.3154e-01, -8.4231e-01,  1.0362e+00, -9.2376e-01,  7.3705e-01],\n",
      "         [-2.4756e+00, -1.2276e+00, -1.0093e+00, -8.9960e-01,  1.3997e-01],\n",
      "         [ 8.1091e-01, -1.3730e+00,  5.9778e-01,  5.6640e-04,  2.7450e-01],\n",
      "         [-1.6370e+00,  1.3457e+00, -8.6471e-01,  1.4560e+00,  5.4191e-01]]])\n",
      "tensor([[[-0.7875,  1.7343],\n",
      "         [ 1.7072, -0.3064],\n",
      "         [ 2.5529,  0.5723],\n",
      "         [ 0.3982, -0.0673]],\n",
      "\n",
      "        [[-0.2493,  0.3355],\n",
      "         [ 1.0274,  1.3147],\n",
      "         [ 0.1264,  1.6612],\n",
      "         [ 2.6297,  0.5045]],\n",
      "\n",
      "        [[ 1.0362,  1.0362],\n",
      "         [-1.0093,  0.1400],\n",
      "         [ 0.8109,  0.5978],\n",
      "         [ 1.3457,  1.4560]]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.MaxPool1d(3, stride = 2)\n",
    "input = torch.randn(3, 4, 5)\n",
    "output = m(input)\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a810ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LSTM(words_train, y_train):\n",
    "    #Iterate through folds\n",
    "\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    local_val_score = 0\n",
    "    models = {}\n",
    "    \n",
    "    nr_classes = 2\n",
    "    print(len(words_train))\n",
    "    print(y_train.shape)\n",
    "\n",
    "    k=0 #initialize fold number\n",
    "    for tr_idx, val_idx in kfold.split(words_train, torch.transpose(y_train, 0,1)):\n",
    "\n",
    "        print('starting fold', k)\n",
    "        k += 1\n",
    "\n",
    "        print(6*'#', 'splitting and reshaping the data')\n",
    "        words_train = np.array(words_train).flatten()\n",
    "        train_input = words_train[tr_idx]\n",
    "        train_target = y_train[:,tr_idx]\n",
    "        val_input = words_train[val_idx]\n",
    "        val_target = y_train[:,val_idx]\n",
    "\n",
    "        print(6*'#', 'Loading')\n",
    "        train = Dataset_Sequential(train_input, train_target)\n",
    "        valid = Dataset_Sequential(val_input, val_target)\n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        #Build tensor data for torch\n",
    "        train_preds = np.zeros((int(train_input.shape[0] * 2)))\n",
    "        val_preds = np.zeros((int(val_input.shape[0] * 2)))\n",
    "        best_val_preds = np.zeros((int(val_input.shape[0] * 2)))\n",
    "        train_targets = np.zeros((int(train_input.shape[0] * 2)))\n",
    "        avg_losses_f = []\n",
    "        avg_val_losses_f = []\n",
    "\n",
    "        #Define loss function\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        #Build model, initialize weights and define optimizer\n",
    "        model = Bi_RNN(lstm_input_size, hidden_state_size, batch_size=batch_size, output_dim=output_dim, num_layers=num_sequence_layers, rnn_type=rnn_type)  # (input_dim, hidden_state_size, batch_size, output_dim, num_seq_layers, rnn_type)\n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)  # Using Adam optimizer\n",
    "        #scheduler = ReduceLROnPlateau(optimizer, 'min', patience=150, factor=0.1, min_lr=1e-8)  # Using ReduceLROnPlateau schedule\n",
    "        temp_val_loss = 9999999999\n",
    "        reached_val_score = 0\n",
    "\n",
    "        #Iterate through epochs\n",
    "        for epoch in range(n_epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            #Train\n",
    "            model.train()\n",
    "            avg_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "                x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n",
    "                y_batch = y_batch.view(-1, num_time_steps, output_dim)\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_batch.cuda())\n",
    "                loss = loss_fn(y_pred.cpu(), y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "                pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "                train_preds[i * batch_size * train_input.shape[1]:(i + 1) * batch_size * train_input.shape[1]] = pred.reshape((-1))\n",
    "                train_targets[i * batch_size * train_input.shape[1]:(i + 1) * batch_size * train_input.shape[1]] = y_batch.detach().cpu().numpy().argmax(axis=2).reshape((-1))\n",
    "                del y_pred, loss, x_batch, y_batch, pred\n",
    "\n",
    "            #Evaluate\n",
    "            model.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n",
    "                y_batch = y_batch.view(-1, num_time_steps, output_dim)\n",
    "                y_pred = model(x_batch.cuda()).detach()\n",
    "                avg_val_loss += loss_fn(y_pred.cpu(), y_batch).item() / len(valid_loader)\n",
    "                pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "                val_preds[i * batch_size * val_input.shape[1]:(i + 1) * batch_size * val_input.shape[1]] = pred.reshape((-1))\n",
    "                del y_pred, x_batch, y_batch, pred\n",
    "            if avg_val_loss < temp_val_loss:\n",
    "                temp_val_loss = avg_val_loss\n",
    "\n",
    "            #Calculate F1-score\n",
    "            train_score = f1_score(train_targets, train_preds, average='macro')\n",
    "            val_score = f1_score(val_target.argmax(axis=2).reshape((-1)), val_preds, average='macro')\n",
    "\n",
    "            #Print output of epoch\n",
    "            elapsed_time = time.time() - start_time\n",
    "            scheduler.step(avg_val_loss)\n",
    "            if epoch%10 == 0:\n",
    "                print('Epoch {}/{} \\t loss={:.4f} \\t train_f1={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} \\t time={:.2f}s'.format(epoch + 1, n_epochs, avg_loss, train_score, avg_val_loss, val_score, elapsed_time))\n",
    "\n",
    "            if val_score > reached_val_score:\n",
    "                reached_val_score = val_score\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "                best_val_preds = copy.deepcopy(val_preds)\n",
    "\n",
    "        #Calculate F1-score of the fold\n",
    "        val_score_fold = f1_score(val_target.argmax(axis=2).reshape((-1)), best_val_preds, average='macro')\n",
    "\n",
    "        #Save the fold's model in a dictionary\n",
    "        models[k] = best_model\n",
    "\n",
    "        #Print F1-score of the fold\n",
    "        print(\"BEST VALIDATION SCORE (F1): \", val_score_fold)\n",
    "        local_val_score += (1/n_folds) * val_score_fold\n",
    "\n",
    "    #Print final average k-fold CV F1-score\n",
    "    print(\"Final Score \", local_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287a434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e170a9f",
   "metadata": {},
   "source": [
    "# Run experiments\n",
    "The following code allows you to run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90dd109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_filename = \"10-million-password-list-top-1000000.txt\"\n",
    "text_filename = \"eng_news_2020_1M-words.txt\"\n",
    "comp_nr_lines = 10000\n",
    "nr_lines = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb2626fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of passwords: 10000\n",
      "Nr of words: 10000\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "passwords, comparison_pw = read_passwords(pw_filename, comp_nr_lines, nr_lines)\n",
    "words = read_words(text_filename, nr_lines)\n",
    "#print(words)\n",
    "print(\"Nr of passwords: {}\\nNr of words: {}\".format(len(passwords), len(words)))\n",
    "\n",
    "all_words = passwords + words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ec1d4",
   "metadata": {},
   "source": [
    "## Feature-based models\n",
    "\n",
    "We have the following feature settings\n",
    "\n",
    "- base_features: simple feature set such as the length of the word and amount of characters from different character sets\n",
    "- levenshtein: calculate the Levensthein distance to most common passwords\n",
    "- ngrams: ngram of characters as features\n",
    "- ngram_range: if ngrams is true, this setting determines which ngrams are to be taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7dcefbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = True\n",
    "levenshtein = False\n",
    "ngrams = False\n",
    "ngram_range = (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec45f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f19278a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# replace next line as soon as we have code that reads in the text file\n",
    "#text = \"Previous research has shown that language and culture influence what passwords look like. Li et al. (2014) compared Chinese and English passwords, and found several differences. First of all, Chinese speakers prefer to use digits while English speakers prefer letters, especially lowercase letters; around 50% of the Chinese passwords were digits only. Besides that, all users use patterns that they are familiar with. In the case of Chinese speakers, this is pinyin words, while in the case of English speakers, these are English words. Finally, they also find that users tend to use date formats that are customary in their culture (e.g. 0104 or 0401 for April 1st). Wang et al. (2019), who also compared Chinese and English passwords, add to this that there are difference in the letter distributions of password groups. For example, Chinese passwords use the letter q more often than English passwords do, likely because q is a common letter in pinyin, but quite rare in English. These distributions do not exactly match the distributions of the languages completely. For example, they found that the letters l and w occurred more often in passwords than in general Chinese pinyin. They argue that this is likely because they are part of the very popular Chinese names Li and Wang. Furthermore, Wang et al. (2019) add that around 11% of Chinese passwords contain a pinyin name (compared to 4% in their English passwords dataset) and about 31% include a 4+ digit date in their password, of which 17% include a 6 digit date. Note that these percentages are likely overestimations as it is not always possible to determine of a number sequence whether it is a date or refers to something else. All these numbers are significantly higher than in the English password set. Phone numbers are also possible passwords. De Tweede Kamerleden waren het er snel over eens: als ze onderzoek wilden doen naar beïnvloeding van moskeeën, dan móésten ze Lorenzo Vidino wel uitnodigen. De gelauwerde extremisme-deskundige uit de Verenigde Staten had er boeken over geschreven, parlementen voorgelicht, regeringen geadviseerd. Dus leek het een logische keuze voor de Kamercommissie die in 2019 onderzoek deed naar buitenlandse moskeefinanciering, om Vidino te horen als expert. Het werd een „heel interessant gesprek”, herinnert een van de commissieleden zich, die anoniem wil blijven omdat de zitting achter gesloten deuren plaatsvond. „Hij schetste een gedetailleerd beeld van de Nederlandse vertakkingen van de Moslimbroederschap.” De islamitische beweging zou hier volgens Vidino „verdeeldheid” zaaien. Hij kon meerdere Nederlandse moskeeën bij naam noemen die onder invloed zouden staan van de Moslimbroeders en wist zelfs de exacte hoogte van ontvangen giften uit Qatar.\"\n",
    "#words = text_to_word_sequence(text, lower=False)\n",
    "\n",
    "# Run feature-based models\n",
    "labels = create_feature_labels(passwords, words)\n",
    "\n",
    "labels_nn = create_nn_labels(passwords, words)\n",
    "print(labels_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff2816b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21274/1647070275.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  total = np.array(list(zip(all_words, features)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8042\n",
      "\n",
      "Confusion matrix:\n",
      "[[811.8 188.2]\n",
      " [203.4 796.6]]\n",
      "--- 4.2651872634887695 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "features = [counts(pw, levenshtein=levenshtein) for pw in passwords]\n",
    "features_word = [counts(word, levenshtein=levenshtein) for word in words]\n",
    "features = np.concatenate((features, features_word), axis=0)\n",
    "\n",
    "\n",
    "if ngrams: \n",
    "    vectorizer = CountVectorizer(analyzer = 'char', lowercase = False, ngram_range = ngram_range)\n",
    "    ngram_features = vectorizer.fit_transform(all_words)\n",
    "    # CountVectorizer returns a sparse matrix. This needs to be converted into a dense matrix in order to be able to concatenate it.\n",
    "    features = np.concatenate((np.array(features), ngram_features.toarray()), axis=1)\n",
    "\n",
    "# link features and words\n",
    "total = np.array(list(zip(all_words, features)))\n",
    "\n",
    "run_ML_model(total, labels, model=\"randomforest\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26f2f3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "torch.Size([2, 20000])\n",
      "starting fold 0\n",
      "###### splitting and reshaping the data\n",
      "###### Loading\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run neural network model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m run_LSTM(all_words, labels_nn)\n",
      "Cell \u001b[0;32mIn [44], line 26\u001b[0m, in \u001b[0;36mrun_LSTM\u001b[0;34m(words_train, y_train)\u001b[0m\n\u001b[1;32m     23\u001b[0m val_target \u001b[38;5;241m=\u001b[39m y_train[:,val_idx]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m6\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mDataset_Sequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m valid \u001b[38;5;241m=\u001b[39m Dataset_Sequential(val_input, val_target)\n\u001b[1;32m     28\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [17], line 4\u001b[0m, in \u001b[0;36mDataset_Sequential.__init__\u001b[0;34m(self, input, output)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, output):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m output\n",
      "File \u001b[0;32m~/Documents/jupyternotebookenvnew/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/jupyternotebookenvnew/lib/python3.10/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/jupyternotebookenvnew/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# Run neural network model\n",
    "run_LSTM(all_words, labels_nn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyternotebookenvnew",
   "language": "python",
   "name": "jupyternotebookenvnew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
