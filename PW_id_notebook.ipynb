{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f4e667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 11:01:32.129787: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-25 11:01:32.200318: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-25 11:01:32.514365: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-25 11:01:32.516785: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-25 11:01:33.238256: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import time\n",
    "\n",
    "from Levenshtein import distance as lev\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d4c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66417da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea4516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67afa464",
   "metadata": {},
   "source": [
    "# Read (pass)words from file\n",
    "Initially we only read the first 100 passwords\n",
    "\n",
    "If you want to read all the lines, use .readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7746d178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1584b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_passwords(filename, comp_nr_lines, nr_lines):\n",
    "    passwords = []\n",
    "    comparison_pw = []\n",
    "\n",
    "    with open(filename, \"r\") as file:\n",
    "        for i in range(comp_nr_lines):\n",
    "            comparison_pw.append(next(file).strip())\n",
    "        for i in range(nr_lines):\n",
    "            passwords.append(next(file).strip())\n",
    "            \n",
    "    return passwords, comparison_pw\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9512c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words(filename, nr_lines):\n",
    "    words = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        for i in range(nr_lines):\n",
    "            words.append(next(file).strip().split(\"\\t\")[1])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa44a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_labels(passwords, words):\n",
    "    return np.concatenate((np.ones(len(passwords)), np.zeros(len(words))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10918d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_labels(passwords, words):\n",
    "    return torch.hstack((torch.concatenate((torch.ones((1,len(passwords))), torch.zeros(1,len(words))), axis=0),\n",
    "                         torch.concatenate((torch.zeros((1,len(passwords))), torch.ones((1,len(words)))), axis=0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61959c3d",
   "metadata": {},
   "source": [
    "Passwords are 1 and nonpassword strings are 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41976a7a",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "Build features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28699350",
   "metadata": {},
   "source": [
    "## Levenshtein\n",
    "To speed up this calculation, it might be worth it to create an implementation of Levenshtein that stops as soon as a distance larger than the current lowest is found. This lowers the average time complexity (but not worst case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "584c6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_levenshtein_distance(word, passwords):\n",
    "    low = 42000\n",
    "    for pw in passwords:\n",
    "        d = lev(word, pw, score_cutoff=low-1)\n",
    "        if d < low:\n",
    "            low = d\n",
    "        if low == 0:\n",
    "            return low\n",
    "    return low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b1b41",
   "metadata": {},
   "source": [
    "## Other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e83f5e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_non_repeating(text):\n",
    "    '''\n",
    "    Remove repeating letters from a string\n",
    "    E.g. aaabbbccccccaaa becomes abca\n",
    "    \n",
    "    text: input text\n",
    "    return: text without repeating letters\n",
    "    '''\n",
    "    count = 0\n",
    "    for i, c in enumerate(text):\n",
    "        if i ==0 or c != text[i-1]:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "def counts(word, levenshtein=False):\n",
    "    alpha_lower = 0\n",
    "    alpha_upper = 0\n",
    "    numeric = 0\n",
    "    special = 0\n",
    "    s = \"\"\n",
    "    for c in word:\n",
    "        if c.islower():\n",
    "            alpha_lower += 1\n",
    "            s += 'L'\n",
    "        elif c.isupper():\n",
    "            alpha_upper += 1\n",
    "            s += 'U'\n",
    "        elif c.isnumeric():\n",
    "            numeric += 1\n",
    "            s += 'N'\n",
    "        else:\n",
    "            special += 1\n",
    "            s += 'S'\n",
    "    length = len(word)\n",
    "    char_sets = bool(alpha_lower) + bool(alpha_upper) + bool(numeric) + bool(special)\n",
    "    if levenshtein: \n",
    "        lev_d = calculate_levenshtein_distance(word, comparison_pw)\n",
    "        return [length, alpha_lower, alpha_lower/length, alpha_upper, alpha_upper/length, numeric, numeric/length, special, special/length, char_sets, count_non_repeating(s), lev_d]\n",
    "    else:\n",
    "        return [length, alpha_lower, alpha_lower/length, alpha_upper, alpha_upper/length, numeric, numeric/length, special, special/length, char_sets, count_non_repeating(s)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc284ea",
   "metadata": {},
   "source": [
    "## ngram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8311d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def word2ngrams(word, n=2):\n",
    "#    return [' ' + word[:n-1]] + [\"\".join(j) for j in zip(*[word[i:] for i in range(n)])] + [word[-(n-1):] + ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "861103c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(word2ngrams(\"test\", 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed7185",
   "metadata": {},
   "source": [
    "# Machine Learning shizzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fde0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "332badc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_errors(y_test, y_pred):\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] != y_pred[i]:\n",
    "            print(words_test[i])\n",
    "            print(y_test[i])\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c259174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_confusion_matrix(matrices):\n",
    "    cumul_matrices = np.zeros(matrices[0].shape)\n",
    "    for matrix in matrices:\n",
    "        cumul_matrices = np.add(cumul_matrices, matrix)\n",
    "    return cumul_matrices / len(matrices)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51433a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ML_model(total, labels, nr_splits = 10, model = \"decisiontree\"):\n",
    "    \n",
    "    accuracies = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    kf = KFold(n_splits = nr_splits, shuffle=True, random_state = 42)\n",
    "    for train, test in kf.split(total):\n",
    "        \n",
    "        X_train_tot, X_test_tot, y_train, y_test = total[train], total[test], labels[train], labels[test]\n",
    "        X_train = [features for _, features in X_train_tot]\n",
    "        words_train = [words for words, _ in X_train_tot]\n",
    "\n",
    "        X_test = [features for _, features in X_test_tot]\n",
    "        words_test = [words for words, _ in X_test_tot]\n",
    "\n",
    "\n",
    "        # pick model\n",
    "        if model == \"decisiontree\":\n",
    "            clf = tree.DecisionTreeClassifier()\n",
    "        elif model == \"randomforest\":\n",
    "            clf = RandomForestClassifier(max_depth = 10, random_state=0)\n",
    "        elif model == \"GaussianNB\":\n",
    "            clf = GaussianNB()\n",
    "        elif model == \"MultinomialNB\":\n",
    "            clf = MultinomialNB()\n",
    "        else:\n",
    "            raise ValueError('Unknown model type supplied: \"{}\". Please specify a different model type'.format(model))\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_pred, y_test))\n",
    "        confusion_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    print(\"Accuracy score: {}\".format(np.mean(accuracies)))\n",
    "\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(average_confusion_matrix(confusion_matrices))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aeb81e",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "447d45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f10de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "lr = 0.01\n",
    "n_folds = 5\n",
    "lstm_input_size = 32\n",
    "hidden_state_size = 256\n",
    "n_layers = 2\n",
    "dropout = 0.125\n",
    "bidirectional = True\n",
    "batch_size = 30\n",
    "num_sequence_layers = 2\n",
    "output_dim = 2                       # !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "num_time_steps = 30                 # !!!!!!!!!!!!!!\n",
    "rnn_type = 'LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f78af523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CharacterDataset object at 0x7f7fd1ec6080>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f7fd1ec6140>\n",
      "[tensor([[21,  4,  2,  0,  4,  3,  0,  1,  1, 12,  0,  2,  1,  3,  2,  2,  1, 13,\n",
      "          3,  2,  0,  3,  1, 14,  8,  9,  5, 10,  6,  4, 11,  9]]), tensor([[0., 0.]])]\n",
      "0 [tensor([[21,  4,  2,  0,  4,  3,  0,  1,  1, 12,  0,  2,  1,  3,  2,  2,  1, 13,\n",
      "          3,  2,  0,  3,  1, 14,  8,  9,  5, 10,  6,  4, 11,  9]]), tensor([[0., 0.]])]\n",
      "1 [tensor([[2, 1, 3, 2, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), tensor([[1., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "class CharacterDataset(Dataset): \n",
    "    \"\"\"Custom dataset. Parameters\n",
    "---------- text : str Input text that will be used to create the entire database. window_size : int\n",
    "Number of characters to use as input features. vocab_size : int Number of characters in the vocabulary.\n",
    "Note that the last character is always reserved for a special \"~\" out-of-vocabulary character. Attributes\n",
    "---------- ch2ix : defaultdict Mapping from the character to the position of that character in the\n",
    "vocabulary. Note that all characters that are not in the vocabulary will get mapped into the index `vocab_size -\n",
    "1`. ix2ch : dict Mapping from the character position in the vocabulary to the actual character.\n",
    "vocabulary : list List of all characters. `len(vocabulary) == vocab_size`. \"\"\" \n",
    "    \n",
    "    def __init__(self, texts, y, max_len=32, vocab_size=25): \n",
    "        self.texts = texts\n",
    "        self.y = y\n",
    "        self.max_len = max_len\n",
    "        self.ch2ix = defaultdict(lambda: vocab_size - 1) \n",
    "        \n",
    "        concat_text = chain.from_iterable(self.texts)\n",
    "        most_common = Counter(concat_text).most_common()\n",
    "        for (c, n) in most_common:\n",
    "            if \"~\" == c:\n",
    "                most_common.remove((\"~\", n))\n",
    "                break\n",
    "        most_common_ch2ix = { \n",
    "            x[0]: i \n",
    "            for i, x in enumerate(most_common[: (vocab_size - 1)]) \n",
    "        }\n",
    "            \n",
    "        self.ch2ix.update(most_common_ch2ix) \n",
    "        self.ch2ix[\"~\"] = vocab_size - 1 \n",
    "        \n",
    "        self.ix2ch = {v: k for k, v in self.ch2ix.items()} \n",
    "        for i in range(len(most_common)-1, vocab_size):\n",
    "            self.ix2ch[i] = \"~\"\n",
    "        self.vocabulary = [self.ix2ch[i] for i in range(vocab_size)] \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, ix): \n",
    "        X = torch.LongTensor(\n",
    "            [self.ch2ix[c] for c in self.texts[ix]] \n",
    "        )\n",
    "        if len(X) > self.max_len:\n",
    "            X = X[:self.max_len]\n",
    "        X = nn.ConstantPad1d((0, self.max_len - len(X)), 0)(X)\n",
    "        y = self.y[ix] \n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    \n",
    "dataset = CharacterDataset([\"Dit is een testtekst self.ch2ix.update(most_common_ch2ix) self.ch2ix[\\\"~\\\"] = vocab_size - 1 self.ix2ch = {v: k for k, v in\", \"test 2\"], \n",
    "                           torch.transpose(torch.tensor([[0., 1., 1., 0., 0., 0.], [0., 0., 0., 1., 1., 1.]]), 0,1), max_len=32, vocab_size=100)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "loader = DataLoader(dataset)\n",
    "\n",
    "print(loader)\n",
    "\n",
    "print(next(iter(loader)))\n",
    "for i, l in enumerate(loader):\n",
    "    print(i, l)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5660778",
   "metadata": {},
   "source": [
    "Do not use anymore, just here for reference\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Dataset_Sequential(Dataset):\n",
    "    def __init__(self, input, output):\n",
    "        self.embedding = nn.Embedding(100,32)\n",
    "        self.input = self.embedding(input)\n",
    "        self.output = output\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input[idx]\n",
    "        y = self.output[idx]\n",
    "        # Embed the input in numbers rather than characters\n",
    "        x = self.embedding(x)\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "        return x, y\n",
    "\n",
    "class Dataset_Sequential_test(Dataset):\n",
    "    def __init__(self, input):\n",
    "        self.embedding = nn.Embedding(100,32)\n",
    "        self.input = self.embedding(input)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input[idx]\n",
    "        # Embed the input in numbers rather than characters\n",
    "        x = self.embedding(x)\n",
    "        x = torch.tensor(x, dtype=torch.str)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6d5e2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=11, vocab_size=100, max_len = 32, num_layers=2, rnn_type='LSTM'):\n",
    "        super(Bi_RNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Define embedding; arguments: num_embeddings, embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, max_len, padding_idx = vocab_size -1)\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = eval('nn.' + rnn_type)(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear1 = nn.Linear(512,128)\n",
    "        self.linear2 = nn.Linear(128,8)\n",
    "        self.linear3 = nn.Linear(8,2)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.maxpool = nn.MaxPool1d(32)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers*2, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers*2, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        input = self.embedding(input)\n",
    "        \n",
    "        # Creating PackedSequence\n",
    "        #packing = nn.utils.rnn.pad_sequence(input)\n",
    "        #                                                  !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # Insert packing stuff\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(input.dtype)\n",
    "        #self.hidden = self.init_hidden()\n",
    "        #print(self.hidden[0].dtype)\n",
    "        \n",
    "        #print(input.shape)\n",
    "        #print(self.hidden[0].shape)\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [batch_size, input_size ,hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both\n",
    "        # have shape (batch_size, num_layers, hidden_dim).\n",
    "        input, self.hidden = self.lstm(input, self.hidden)\n",
    "        #print(self.hidden[0].shape)\n",
    "        #print(input.shape)\n",
    "        \n",
    "        # Unpacking PackedSequence\n",
    "        #                                                  !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # Insert packing stuff\n",
    "        \n",
    "        \n",
    "        # Something with Permute???????                    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        input = torch.permute(input, (0,2,1))\n",
    "        \n",
    "        # Tanh\n",
    "        input = self.tanh(input)\n",
    "        \n",
    "        # MaxPool 1D\n",
    "        input = self.maxpool(input)\n",
    "        #print(input.shape)\n",
    "        \n",
    "        # Tanh\n",
    "        input = self.tanh(input)\n",
    "        #print(input.shape)\n",
    "        \n",
    "        # Squeeze\n",
    "        input = torch.squeeze(input)\n",
    "        #print(input.shape)\n",
    "        \n",
    "        # First linear layer\n",
    "        input = self.linear1(input)\n",
    "        \n",
    "        # Dropout\n",
    "        \n",
    "        # Second linear layer\n",
    "        input = self.linear2(input)\n",
    "        \n",
    "        # Dropout\n",
    "        \n",
    "        # Third linear layer\n",
    "        output = self.linear3(input)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a8c86c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    test = Dataset_Sequential_test(X_test)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb06c602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1474, -0.4180,  0.8995, -1.4237,  0.5044],\n",
      "         [-0.7584,  1.2553,  0.9250, -0.3608,  1.0974],\n",
      "         [-1.4276,  0.9039, -0.2039, -0.6851, -0.6653],\n",
      "         [ 0.9765, -0.9886,  0.2896, -0.9779,  0.3823]],\n",
      "\n",
      "        [[ 0.5414, -0.3590, -0.1734, -1.6161, -0.1813],\n",
      "         [ 0.6886, -0.4078, -0.1343,  0.2776,  1.0186],\n",
      "         [-0.3481, -0.2074, -0.4273, -0.8174,  0.2781],\n",
      "         [ 0.1157,  0.6015, -1.7914, -0.8316,  0.1088]],\n",
      "\n",
      "        [[ 0.5532, -0.9134, -0.9747, -0.1189,  2.8603],\n",
      "         [ 0.3371, -0.2142, -0.4060, -1.0705, -0.9318],\n",
      "         [ 0.0784, -0.6396,  0.5651,  0.1036,  1.1783],\n",
      "         [-1.2281,  0.6504,  0.4652, -0.2746,  2.1576]]])\n",
      "tensor([[[ 0.8995,  0.8995],\n",
      "         [ 1.2553,  1.0974],\n",
      "         [ 0.9039, -0.2039],\n",
      "         [ 0.9765,  0.3823]],\n",
      "\n",
      "        [[ 0.5414, -0.1734],\n",
      "         [ 0.6886,  1.0186],\n",
      "         [-0.2074,  0.2781],\n",
      "         [ 0.6015,  0.1088]],\n",
      "\n",
      "        [[ 0.5532,  2.8603],\n",
      "         [ 0.3371, -0.4060],\n",
      "         [ 0.5651,  1.1783],\n",
      "         [ 0.6504,  2.1576]]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.MaxPool1d(3, stride = 2)\n",
    "input = torch.randn(3, 4, 5)\n",
    "output = m(input)\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a810ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LSTM(words_train, y_train):\n",
    "    #Iterate through folds\n",
    "\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    local_val_score = 0\n",
    "    models = {}\n",
    "    \n",
    "    y_train = torch.transpose(y_train, 0, 1)\n",
    "    nr_classes = 2\n",
    "    vocab_size = 25\n",
    "    print(len(words_train))\n",
    "    print(y_train.shape)\n",
    "\n",
    "    k=0 #initialize fold number\n",
    "    for tr_idx, val_idx in kfold.split(words_train, y_train):\n",
    "\n",
    "        print('starting fold', k)\n",
    "        k += 1\n",
    "\n",
    "        print(6*'#', 'splitting and reshaping the data')\n",
    "        words_train = np.array(words_train).flatten()\n",
    "        train_input = words_train[tr_idx]\n",
    "        train_target = y_train[tr_idx]\n",
    "        val_input = words_train[val_idx]\n",
    "        val_target = y_train[val_idx]\n",
    "\n",
    "        print(6*'#', 'Loading')\n",
    "        train = CharacterDataset(train_input, train_target, vocab_size=vocab_size)\n",
    "        valid = CharacterDataset(val_input, val_target, vocab_size=vocab_size)\n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False, drop_last = True)\n",
    "\n",
    "        #Build tensor data for torch\n",
    "        train_preds = np.zeros((int(train_input.shape[0] * 2)))\n",
    "        val_preds = np.zeros((int(val_input.shape[0] * 2)))\n",
    "        best_val_preds = np.zeros((int(val_input.shape[0] * 2)))\n",
    "        train_targets = np.zeros((int(train_input.shape[0] * 2)))\n",
    "        avg_losses_f = []\n",
    "        avg_val_losses_f = []\n",
    "\n",
    "        #Define loss function\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        #Build model, initialize weights and define optimizer\n",
    "        model = Bi_RNN(lstm_input_size, hidden_state_size, batch_size=batch_size, output_dim=output_dim, num_layers=num_sequence_layers, rnn_type=rnn_type)  # (input_dim, hidden_state_size, batch_size, output_dim, num_seq_layers, rnn_type)\n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)  # Using Adam optimizer\n",
    "        #scheduler = ReduceLROnPlateau(optimizer, 'min', patience=150, factor=0.1, min_lr=1e-8)  # Using ReduceLROnPlateau schedule\n",
    "        temp_val_loss = 9999999999\n",
    "        reached_val_score = 0\n",
    "\n",
    "        #Iterate through epochs\n",
    "        for epoch in range(n_epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            #Train\n",
    "            model.train()\n",
    "            avg_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "                print(i)\n",
    "                #x_batch = x_batch.reshape(-1, num_time_steps, lstm_input_size)\n",
    "                #y_batch = y_batch.view(-1, num_time_steps, output_dim)\n",
    "                print(x_batch.shape)\n",
    "                #print(\"x_batch \", x_batch)\n",
    "                #features = torch.LongTensor([[dataset.ch2ix[c] for c in x_batch]])\n",
    "                #print(\"features \",features)\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_batch.cpu())\n",
    "                loss = loss_fn(y_pred.cpu(), y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "                \n",
    "                #pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "                # LATER FIXEN !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                #train_preds[i * batch_size * train_input.shape[1]:(i + 1) * batch_size * train_input.shape[1]] = y_pred.reshape((-1))\n",
    "                #train_targets[i * batch_size * train_input.shape[1]:(i + 1) * batch_size * train_input.shape[1]] = y_batch.detach().cpu().numpy().argmax(axis=2).reshape((-1))\n",
    "                \n",
    "                del y_pred, loss, x_batch, y_batch\n",
    "\n",
    "            #Evaluate\n",
    "            model.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                \n",
    "                x_batch = x_batch.reshape(-1, num_time_steps, lstm_input_size)\n",
    "                y_batch = y_batch.view(-1, num_time_steps, output_dim)\n",
    "                y_pred = model(x_batch.cpu()).detach()\n",
    "                avg_val_loss += loss_fn(y_pred.cpu(), y_batch).item() / len(valid_loader)\n",
    "                pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "                val_preds[i * batch_size * val_input.shape[1]:(i + 1) * batch_size * val_input.shape[1]] = pred.reshape((-1))\n",
    "                del y_pred, x_batch, y_batch, pred\n",
    "            if avg_val_loss < temp_val_loss:\n",
    "                temp_val_loss = avg_val_loss\n",
    "\n",
    "            #Calculate F1-score\n",
    "            train_score = f1_score(train_targets, train_preds, average='macro')\n",
    "            val_score = f1_score(val_target.argmax(axis=2).reshape((-1)), val_preds, average='macro')\n",
    "\n",
    "            #Print output of epoch\n",
    "            elapsed_time = time.time() - start_time\n",
    "            scheduler.step(avg_val_loss)\n",
    "            if epoch%10 == 0:\n",
    "                print('Epoch {}/{} \\t loss={:.4f} \\t train_f1={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} \\t time={:.2f}s'.format(epoch + 1, n_epochs, avg_loss, train_score, avg_val_loss, val_score, elapsed_time))\n",
    "\n",
    "            if val_score > reached_val_score:\n",
    "                reached_val_score = val_score\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "                best_val_preds = copy.deepcopy(val_preds)\n",
    "\n",
    "        #Calculate F1-score of the fold\n",
    "        val_score_fold = f1_score(val_target.argmax(axis=2).reshape((-1)), best_val_preds, average='macro')\n",
    "\n",
    "        #Save the fold's model in a dictionary\n",
    "        models[k] = best_model\n",
    "\n",
    "        #Print F1-score of the fold\n",
    "        print(\"BEST VALIDATION SCORE (F1): \", val_score_fold)\n",
    "        local_val_score += (1/n_folds) * val_score_fold\n",
    "\n",
    "    #Print final average k-fold CV F1-score\n",
    "    print(\"Final Score \", local_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287a434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e170a9f",
   "metadata": {},
   "source": [
    "# Run experiments\n",
    "The following code allows you to run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "90dd109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_filename = \"10-million-password-list-top-1000000.txt\"\n",
    "text_filename = \"eng_news_2020_1M-words.txt\"\n",
    "comp_nr_lines = 10000\n",
    "nr_lines = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bb2626fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of passwords: 90\n",
      "Nr of words: 90\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "passwords, comparison_pw = read_passwords(pw_filename, comp_nr_lines, nr_lines)\n",
    "words = read_words(text_filename, nr_lines)\n",
    "#print(words)\n",
    "print(\"Nr of passwords: {}\\nNr of words: {}\".format(len(passwords), len(words)))\n",
    "\n",
    "all_words = passwords + words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ec1d4",
   "metadata": {},
   "source": [
    "## Feature-based models\n",
    "\n",
    "We have the following feature settings\n",
    "\n",
    "- base_features: simple feature set such as the length of the word and amount of characters from different character sets\n",
    "- levenshtein: calculate the Levensthein distance to most common passwords\n",
    "- ngrams: ngram of characters as features\n",
    "- ngram_range: if ngrams is true, this setting determines which ngrams are to be taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7dcefbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = True\n",
    "levenshtein = False\n",
    "ngrams = False\n",
    "ngram_range = (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec45f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f19278a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# replace next line as soon as we have code that reads in the text file\n",
    "#text = \"Previous research has shown that language and culture influence what passwords look like. Li et al. (2014) compared Chinese and English passwords, and found several differences. First of all, Chinese speakers prefer to use digits while English speakers prefer letters, especially lowercase letters; around 50% of the Chinese passwords were digits only. Besides that, all users use patterns that they are familiar with. In the case of Chinese speakers, this is pinyin words, while in the case of English speakers, these are English words. Finally, they also find that users tend to use date formats that are customary in their culture (e.g. 0104 or 0401 for April 1st). Wang et al. (2019), who also compared Chinese and English passwords, add to this that there are difference in the letter distributions of password groups. For example, Chinese passwords use the letter q more often than English passwords do, likely because q is a common letter in pinyin, but quite rare in English. These distributions do not exactly match the distributions of the languages completely. For example, they found that the letters l and w occurred more often in passwords than in general Chinese pinyin. They argue that this is likely because they are part of the very popular Chinese names Li and Wang. Furthermore, Wang et al. (2019) add that around 11% of Chinese passwords contain a pinyin name (compared to 4% in their English passwords dataset) and about 31% include a 4+ digit date in their password, of which 17% include a 6 digit date. Note that these percentages are likely overestimations as it is not always possible to determine of a number sequence whether it is a date or refers to something else. All these numbers are significantly higher than in the English password set. Phone numbers are also possible passwords. De Tweede Kamerleden waren het er snel over eens: als ze onderzoek wilden doen naar beïnvloeding van moskeeën, dan móésten ze Lorenzo Vidino wel uitnodigen. De gelauwerde extremisme-deskundige uit de Verenigde Staten had er boeken over geschreven, parlementen voorgelicht, regeringen geadviseerd. Dus leek het een logische keuze voor de Kamercommissie die in 2019 onderzoek deed naar buitenlandse moskeefinanciering, om Vidino te horen als expert. Het werd een „heel interessant gesprek”, herinnert een van de commissieleden zich, die anoniem wil blijven omdat de zitting achter gesloten deuren plaatsvond. „Hij schetste een gedetailleerd beeld van de Nederlandse vertakkingen van de Moslimbroederschap.” De islamitische beweging zou hier volgens Vidino „verdeeldheid” zaaien. Hij kon meerdere Nederlandse moskeeën bij naam noemen die onder invloed zouden staan van de Moslimbroeders en wist zelfs de exacte hoogte van ontvangen giften uit Qatar.\"\n",
    "#words = text_to_word_sequence(text, lower=False)\n",
    "\n",
    "# Run feature-based models\n",
    "labels = create_feature_labels(passwords, words)\n",
    "\n",
    "labels_nn = create_nn_labels(passwords, words)\n",
    "print(labels_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ff2816b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 1.0\n",
      "\n",
      "Confusion matrix:\n",
      "[[18.]]\n",
      "--- 0.017612934112548828 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5003/4249568652.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  total = np.array(list(zip(all_words, features)))\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "features = [counts(pw, levenshtein=levenshtein) for pw in passwords]\n",
    "features_word = [counts(word, levenshtein=levenshtein) for word in words]\n",
    "features = np.concatenate((features, features_word), axis=0)\n",
    "\n",
    "\n",
    "if ngrams: \n",
    "    vectorizer = CountVectorizer(analyzer = 'char', lowercase = False, ngram_range = ngram_range)\n",
    "    ngram_features = vectorizer.fit_transform(all_words)\n",
    "    # CountVectorizer returns a sparse matrix. This needs to be converted into a dense matrix in order to be able to concatenate it.\n",
    "    features = np.concatenate((np.array(features), ngram_features.toarray()), axis=1)\n",
    "\n",
    "# link features and words\n",
    "total = np.array(list(zip(all_words, features)))\n",
    "\n",
    "run_ML_model(total, labels, model=\"decisiontree\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d4ba66f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 30, 32])\n",
      "torch.Size([1, 30, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch = torch.Tensor(1, 30, 32)\n",
    "print(x_batch.shape)\n",
    "x_batch = x_batch.reshape(-1, num_time_steps, lstm_input_size)\n",
    "print(x_batch.shape)\n",
    "\n",
    "m = nn.MaxPool1d(3)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "26f2f3d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "torch.Size([180, 2])\n",
      "starting fold 0\n",
      "###### splitting and reshaping the data\n",
      "###### Loading\n",
      "0\n",
      "torch.Size([30, 32])\n",
      "1\n",
      "torch.Size([30, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run neural network model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_LSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_nn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# NIET VERGETEN:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#   Dropout toevoegen\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[115], line 70\u001b[0m, in \u001b[0;36mrun_LSTM\u001b[0;34m(words_train, y_train)\u001b[0m\n\u001b[1;32m     68\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(x_batch\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     69\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred\u001b[38;5;241m.\u001b[39mcpu(), y_batch)\n\u001b[0;32m---> 70\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     72\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/Documents/datasciencevenv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/datasciencevenv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# Run neural network model\n",
    "run_LSTM(all_words, labels_nn)\n",
    "\n",
    "# NIET VERGETEN:\n",
    "#   Dropout toevoegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20ae2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.\n",
    "# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n",
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]\n",
    "# Print the first 3, just so you can see what they look like.\n",
    "print(ngrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!\n",
    "\n",
    "# To get the embedding of a particular word, e.g. \"beauty\"\n",
    "print(model.embeddings.weight[word_to_ix[\"beauty\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python DataScienceVenv",
   "language": "python",
   "name": "datasciencevenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
