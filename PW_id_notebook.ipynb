{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f4e667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 08:22:33.945335: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-12 08:22:34.104086: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-07-12 08:22:34.104102: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-07-12 08:22:34.812529: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-12 08:22:34.812603: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-12 08:22:34.812610: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d4c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66417da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67afa464",
   "metadata": {},
   "source": [
    "# Read passwords from file\n",
    "Initially we only read the first 100 passwords\n",
    "\n",
    "If you want to read all the lines, use .readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7746d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"darkweb2017-top10000.txt\"\n",
    "nr_lines = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1584b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "passwords = []\n",
    "comparison_pw = []\n",
    "\n",
    "with open(filename, \"r\") as file:\n",
    "    for i in range(nr_lines):\n",
    "        passwords.append(next(file).strip())\n",
    "        comparison_pw.append(next(file).strip())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9512c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123456', '111111', 'qwerty', '12345678', '1234567', '1234567890', '12345', '1q2w3e4r5t', '123456a', 'monkey', 'dragon', '666666', 'myspace1', '121212', '123qwe', 'tinkle', 'gwerty', 'gwerty123', '7777777', '1q2w3e4r', '222222', 'qwerty123', '555555', 'fuckyou', '12345a', '1q2w3e', 'computer', '159753', 'fuckyou1', '789456123', '123654', '888888', 'michael', 'football', '777777', '999999', 'monkey1', 'daniel', 'a12345', '789456', 'love123', 'jordan23', '5201314', 'FQRG7CS493', 'asdf', 'superman', 'love', 'shadow', '333333', 'babygirl1', 'qwert', 'baseball', '0', 'soccer', '131313', '1111111', '0123456789', 'iloveyou2', 'jordan', 'bitch1', 'q1w2e3r4', 'qwer1234', 'soccer1', '101010', 'thomas', 'fuckyou2', 'nicole', '1', 'qazwsxedc', 'andrew', 'apple', 'anthony1', 'money1', 'abc', 'anthony', 'jennifer', 'naruto', '696969', 'joshua', '29rsavoy', 'andrea', 'qwerty12', 'passw0rd', 'hunter', 'welcome', 'superman1', 'xbox360', 'ashley1', 'babygirl', 'trustno1', 'asdf1234', 'buster', 'tigger', 'freedom', 'matthew', 'password2', 'george', '12341234', 'hannah', 'friends', 'william', 'samantha', 'nicole1', 'robert', 'jordan1', 'letmein', '212121', '$HEX', 'batman', 'a123456789', 'qweqwe', '232323', 'martin', 'forever', '1qazxsw2', 'cjmasterinf', 'harley', 'brandon1', '1234567891', 'chris1', 'abcdef', '1342', 'loveyou', 'wall.e', '12413', 'PE#5GZ29PTZMSE', 'dpbk1234', 'U38fa39', 'cookie', 'jasmine', '12345q', 'pakistan', '123789', 'joseph', 'ginger', 'matthew1', 'justin1', '3rJs1la7qE', 'antonio', 'matrix', 'hottie1', 'sandra', '12345678910', 'arsenal', 'brandon', 'jonathan', 'liverpool1', 'ghbdtn', 'mercedes', '11223344', '456789', 'asshole', 'qwertyu', 'red123', 'eminem', '111222tianya', 'william1', 'angel', 'richard', 'banana', 'jasmine1', 'welcome1', 'hunter1', 'melissa', 'christian', 'oliver', 'butterfly1', '55555', 'mylove', 'NULL', '1234561', 'america', 'monster', '456123', 'slipknot', 'zaq12wsx', '147852369', 'elizabeth', 'Status', 'robert1', 'nathan', 'buddy1', 'america1', 'chelsea1', 'prince', 'jackson', 'rainbow', '1234567a', 'iw14Fi9j', 'juventus', '!~!1', 'samuel', 'lovers', '0123456', '999999999', 'elizabeth1', 'buster1', 'david1', '123qweasd', 'carlos', 'samantha1', 'joshua1', 'stella', 'asdasd5', 'whatever1', '00000', 'a1b2c3', 'austin', 'qaz123', 'music1', 'family1', 'steven', '1234abcd', 'thomas1', 'madison1', 'summer1', 'nicholas', '123456m', 'spiderman', 'diamond1', 'danielle', '7758521', 'pokemon1', '1qaz2wsx3edc', 'loulou', 'yamaha', 'scooter', 'tennis', 'i', 'friends1', 'maggie1', 'qwerty12345', 'jesus', 'nicolas', 'weed420', 'loser1', 'iloveyou!', 'fuckoff1', 'iloveu2', 'pussy1', '098765', 'smokey', '123456789q', 'heather', 'booboo', '4815162342', 'chester', '123456b', 'edward', 'canada', 'destiny', 'nigger1', 'asdfghjkl1', 'casper', 'mother1', 'qazxsw', '1q2w3e4r5t6y', 'money', 'patrick1', '12121212', 'raiders1', 'sebastian', 'zxcvbnm1', '852456', 'daniela', 'olivia', '010101', 'spiderman1', '0000000', '741852', 'a1234567', '123456d', 'apple1', 'alexandra', 'iloveu1', 'fuckme1', 'yankees', 'cristina', 'jackson1', 'friend', 'sammy1', 'phoenix', 'rocky1', 'joseph1', 'p', 'richard1', 'mickey1', 'j123456', 'newyork', 'charles', 'orange1', '421uiopy258', 'cameron', 'barbie', 'vincent', 'scorpion', 'aaaaa', 'asdf123', 'zk.:', 'fucker1', 'hotmail', 'doudou', 'bailey1', 'fucker', 'sparky', '123456abc', 'booboo1', '9876543210', 'midnight', 'jessie', 'austin1', 'pass', 'claudia', 'kristina', 'lovelove', 'tiger1', 'dolphin', 'gangsta1', '151515', 'scooter1', 'fuck', 'junior1', 'scooby', 'aaaa', 'kitty1', 'beautiful', 'danielle1', 'skater1', 'qazwsx123', 'b123456', 'guitar1', 'peaches', 'sakura', 'soleil', 'green1', 'cooper', 'muffin', 'love13', 'arsenal1', 'diablo', 'george1', 'crystal', 'player1', 'vfhbyf', 'Password', 'chivas1', 'hockey1', 'pussy', 'stalker', 'tweety', 'creative', 'pretty1', 'maverick', 'nathan1', 'cameron1', 'google1', 'martina', 'spongebob', 'fernando', 'startfinding', 'dolphin1', 'test123', 'kobe24', 'adrian', 'aaaaaa1', 'isabella', 'password3', 'abcdefg123', 'shannon', 'manuel', 'molly1', '123456z', 'password.', 'miguel', 'sergey', 'abc1234', 'qwert123', 'poohbear', 'school1', '951753', '111', 'snoopy1', 'YAgjecc826', 'candy1', 'qwerty123456', 'eminem1', '789789', 'steelers', 'morgan1', 'boomer', 'nastya', 'carmen', 'nicholas1', 'precious', 'jonathan1', 'bitch', 'rabbit', 'angel123', 'barbara', 'fuckyou!', 'barney', 'hiphop', 'shorty', 'simone', 'marlboro', 'cowboys', 'alex', '1234512345', 'qq123456', 'bond007', 'eagles', 'azertyuiop', 'sexy12', 'james', 'fatima', 'icecream', '121314', 'qazwsx1', 'twilight', '9379992', 'dancer', 'beauty', 'maxwell', 'dexter', 'qazqaz', 'love11', 'aaaaaaaa', 'fyfcnfcbz', 'aaa111', 'hottie', 'alyssa', 'lovers1', 'alicia', 'blue123', 'ranger', 'pumpkin', 'edward1', 'christine', '54321', 'marie1', 'steelers1', 'shannon1', 'cutie1', 'florida1', 'stephanie1', 'cassie', 'rachel1', 'krishna', 'october', 'motorola', 'hahaha1', 'lakers24', 'andrey', 'turtle', 'baby', 'pa55word', 'emmanuel', '012345', '5555555555', 'karina', 'musica', 'love4ever', 'greenday', 'nothing', 'chicago', 'mnbvcxz', '242424', 'santiago', 'kevin1', 'chester1', 'kimberly', 'z123456', 'jackass', '5555555', 'boston', '55555555', '111111a', '090909', 'horses', 'qwer', 'redsox', 'a12345678', 'tucker', 'corvette', 'realmadrid', 'rangers', '1123581321', 'sayang', 'christ', 'fktrcfylh', 'player', 'qwert12345', 'trinity', 'p@ssw0rd', 'zxcvbnm123', 'lebron23', 'strawberry', 'love1234', 'asdfg1', 'soccer10']\n"
     ]
    }
   ],
   "source": [
    "print(passwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9be48c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Previous research has shown that language and culture influence what passwords look like. Li et al. (2014) compared Chinese and English passwords, and found several differences. First of all, Chinese speakers prefer to use digits while English speakers prefer letters, especially lowercase letters; around 50% of the Chinese passwords were digits only. Besides that, all users use patterns that they are familiar with. In the case of Chinese speakers, this is pinyin words, while in the case of English speakers, these are English words. Finally, they also find that users tend to use date formats that are customary in their culture (e.g. 0104 or 0401 for April 1st). Wang et al. (2019), who also compared Chinese and English passwords, add to this that there are difference in the letter distributions of password groups. For example, Chinese passwords use the letter q more often than English passwords do, likely because q is a common letter in pinyin, but quite rare in English. These distributions do not exactly match the distributions of the languages completely. For example, they found that the letters l and w occurred more often in passwords than in general Chinese pinyin. They argue that this is likely because they are part of the very popular Chinese names Li and Wang. Furthermore, Wang et al. (2019) add that around 11% of Chinese passwords contain a pinyin name (compared to 4% in their English passwords dataset) and about 31% include a 4+ digit date in their password, of which 17% include a 6 digit date. Note that these percentages are likely overestimations as it is not always possible to determine of a number sequence whether it is a date or refers to something else. All these numbers are significantly higher than in the English password set. Phone numbers are also possible passwords. De Tweede Kamerleden waren het er snel over eens: als ze onderzoek wilden doen naar beïnvloeding van moskeeën, dan móésten ze Lorenzo Vidino wel uitnodigen. De gelauwerde extremisme-deskundige uit de Verenigde Staten had er boeken over geschreven, parlementen voorgelicht, regeringen geadviseerd. Dus leek het een logische keuze voor de Kamercommissie die in 2019 onderzoek deed naar buitenlandse moskeefinanciering, om Vidino te horen als expert. Het werd een „heel interessant gesprek”, herinnert een van de commissieleden zich, die anoniem wil blijven omdat de zitting achter gesloten deuren plaatsvond. „Hij schetste een gedetailleerd beeld van de Nederlandse vertakkingen van de Moslimbroederschap.” De islamitische beweging zou hier volgens Vidino „verdeeldheid” zaaien. Hij kon meerdere Nederlandse moskeeën bij naam noemen die onder invloed zouden staan van de Moslimbroeders en wist zelfs de exacte hoogte van ontvangen giften uit Qatar.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "638d3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### !!!!!!!!!!!!!!!!!!!!! This is not ideal, this decapitalizes words, so preferably use something else for preprocessing\n",
    "\n",
    "words = text_to_word_sequence(text, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aec45f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440\n",
      "['Previous', 'research', 'has', 'shown', 'that', 'language', 'and', 'culture', 'influence', 'what', 'passwords', 'look', 'like', 'Li', 'et', 'al', '2014', 'compared', 'Chinese', 'and', 'English', 'passwords', 'and', 'found', 'several', 'differences', 'First', 'of', 'all', 'Chinese', 'speakers', 'prefer', 'to', 'use', 'digits', 'while', 'English', 'speakers', 'prefer', 'letters', 'especially', 'lowercase', 'letters', 'around', '50', 'of', 'the', 'Chinese', 'passwords', 'were', 'digits', 'only', 'Besides', 'that', 'all', 'users', 'use', 'patterns', 'that', 'they', 'are', 'familiar', 'with', 'In', 'the', 'case', 'of', 'Chinese', 'speakers', 'this', 'is', 'pinyin', 'words', 'while', 'in', 'the', 'case', 'of', 'English', 'speakers', 'these', 'are', 'English', 'words', 'Finally', 'they', 'also', 'find', 'that', 'users', 'tend', 'to', 'use', 'date', 'formats', 'that', 'are', 'customary', 'in', 'their', 'culture', 'e', 'g', '0104', 'or', '0401', 'for', 'April', '1st', 'Wang', 'et', 'al', '2019', 'who', 'also', 'compared', 'Chinese', 'and', 'English', 'passwords', 'add', 'to', 'this', 'that', 'there', 'are', 'difference', 'in', 'the', 'letter', 'distributions', 'of', 'password', 'groups', 'For', 'example', 'Chinese', 'passwords', 'use', 'the', 'letter', 'q', 'more', 'often', 'than', 'English', 'passwords', 'do', 'likely', 'because', 'q', 'is', 'a', 'common', 'letter', 'in', 'pinyin', 'but', 'quite', 'rare', 'in', 'English', 'These', 'distributions', 'do', 'not', 'exactly', 'match', 'the', 'distributions', 'of', 'the', 'languages', 'completely', 'For', 'example', 'they', 'found', 'that', 'the', 'letters', 'l', 'and', 'w', 'occurred', 'more', 'often', 'in', 'passwords', 'than', 'in', 'general', 'Chinese', 'pinyin', 'They', 'argue', 'that', 'this', 'is', 'likely', 'because', 'they', 'are', 'part', 'of', 'the', 'very', 'popular', 'Chinese', 'names', 'Li', 'and', 'Wang', 'Furthermore', 'Wang', 'et', 'al', '2019', 'add', 'that', 'around', '11', 'of', 'Chinese', 'passwords', 'contain', 'a', 'pinyin', 'name', 'compared', 'to', '4', 'in', 'their', 'English', 'passwords', 'dataset', 'and', 'about', '31', 'include', 'a', '4', 'digit', 'date', 'in', 'their', 'password', 'of', 'which', '17', 'include', 'a', '6', 'digit', 'date', 'Note', 'that', 'these', 'percentages', 'are', 'likely', 'overestimations', 'as', 'it', 'is', 'not', 'always', 'possible', 'to', 'determine', 'of', 'a', 'number', 'sequence', 'whether', 'it', 'is', 'a', 'date', 'or', 'refers', 'to', 'something', 'else', 'All', 'these', 'numbers', 'are', 'significantly', 'higher', 'than', 'in', 'the', 'English', 'password', 'set', 'Phone', 'numbers', 'are', 'also', 'possible', 'passwords', 'De', 'Tweede', 'Kamerleden', 'waren', 'het', 'er', 'snel', 'over', 'eens', 'als', 'ze', 'onderzoek', 'wilden', 'doen', 'naar', 'beïnvloeding', 'van', 'moskeeën', 'dan', 'móésten', 'ze', 'Lorenzo', 'Vidino', 'wel', 'uitnodigen', 'De', 'gelauwerde', 'extremisme', 'deskundige', 'uit', 'de', 'Verenigde', 'Staten', 'had', 'er', 'boeken', 'over', 'geschreven', 'parlementen', 'voorgelicht', 'regeringen', 'geadviseerd', 'Dus', 'leek', 'het', 'een', 'logische', 'keuze', 'voor', 'de', 'Kamercommissie', 'die', 'in', '2019', 'onderzoek', 'deed', 'naar', 'buitenlandse', 'moskeefinanciering', 'om', 'Vidino', 'te', 'horen', 'als', 'expert', 'Het', 'werd', 'een', '„heel', 'interessant', 'gesprek”', 'herinnert', 'een', 'van', 'de', 'commissieleden', 'zich', 'die', 'anoniem', 'wil', 'blijven', 'omdat', 'de', 'zitting', 'achter', 'gesloten', 'deuren', 'plaatsvond', '„Hij', 'schetste', 'een', 'gedetailleerd', 'beeld', 'van', 'de', 'Nederlandse', 'vertakkingen', 'van', 'de', 'Moslimbroederschap', '”', 'De', 'islamitische', 'beweging', 'zou', 'hier', 'volgens', 'Vidino', '„verdeeldheid”', 'zaaien', 'Hij', 'kon', 'meerdere', 'Nederlandse', 'moskeeën', 'bij', 'naam', 'noemen', 'die', 'onder', 'invloed', 'zouden', 'staan', 'van', 'de', 'Moslimbroeders', 'en', 'wist', 'zelfs', 'de', 'exacte', 'hoogte', 'van', 'ontvangen', 'giften', 'uit', 'Qatar']\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41976a7a",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "Build features\n",
    "\n",
    "- base_features: simple feature set such as the length of the word and amount of characters from different character sets\n",
    "- levenshtein: calculate the Levensthein distance to most common passwords\n",
    "- ngrams: ngram of characters as features\n",
    "- ngram_range: if ngrams is true, this setting determines which ngrams are to be taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ae7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = True\n",
    "levenshtein = True\n",
    "ngrams = True\n",
    "ngram_range = (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa44a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.concatenate((np.ones(len(features)), np.zeros(len(features_word))), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61959c3d",
   "metadata": {},
   "source": [
    "Passwords are 1 and nonpassword strings are 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc284d",
   "metadata": {},
   "source": [
    "## Levenshtein\n",
    "To speed up this calculation, it might be worth it to create an implementation of Levenshtein that stops as soon as a distance larger than the current lowest is found. This increases the average time complexity (but not worst case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6b3379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89a6d5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lev(\"hoi\", \"hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d41f75a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_levenshtein_distance(word, passwords):\n",
    "    low = 42000\n",
    "    for pw in passwords:\n",
    "        d = lev(word, pw)\n",
    "        if d < low:\n",
    "            low = d\n",
    "    return low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a75847",
   "metadata": {},
   "source": [
    "## Other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e83f5e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_non_repeating(text):\n",
    "    '''\n",
    "    Remove repeating letters from a string\n",
    "    E.g. aaabbbccccccaaa becomes abca\n",
    "    \n",
    "    text: input text\n",
    "    return: text without repeating letters\n",
    "    '''\n",
    "    count = 0\n",
    "    for i, c in enumerate(text):\n",
    "        if i ==0 or c != text[i-1]:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "def counts(word):\n",
    "    alpha_lower = 0\n",
    "    alpha_upper = 0\n",
    "    numeric = 0\n",
    "    special = 0\n",
    "    s = \"\"\n",
    "    for c in word:\n",
    "        if c.islower():\n",
    "            alpha_lower += 1\n",
    "            s += 'L'\n",
    "        elif c.isupper():\n",
    "            alpha_upper += 1\n",
    "            s += 'U'\n",
    "        elif c.isnumeric():\n",
    "            numeric += 1\n",
    "            s += 'N'\n",
    "        else:\n",
    "            special += 1\n",
    "            s += 'S'\n",
    "    length = len(word)\n",
    "    char_sets = bool(alpha_lower) + bool(alpha_upper) + bool(numeric) + bool(special)\n",
    "    lev_d = calculate_levenshtein_distance(word, comparison_pw)\n",
    "    return [length, alpha_lower, alpha_lower/length, alpha_upper, alpha_upper/length, numeric, numeric/length, special, special/length, char_sets, count_non_repeating(s), lev_d]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff2816b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [counts(pw) for pw in passwords]\n",
    "#print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23e77e01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_word = [counts(word) for word in words]\n",
    "#print(features_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4fe3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate((features, features_word), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4c0a030",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.  0.  0.  ... 1.  1.  1. ]\n",
      " [6.  0.  0.  ... 1.  1.  1. ]\n",
      " [6.  6.  1.  ... 1.  1.  1. ]\n",
      " ...\n",
      " [6.  6.  1.  ... 1.  1.  2. ]\n",
      " [3.  3.  1.  ... 1.  1.  3. ]\n",
      " [5.  4.  0.8 ... 2.  2.  3. ]]\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc284ea",
   "metadata": {},
   "source": [
    "## ngram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8311d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2ngrams(word, n=2):\n",
    "    return [' ' + word[:n-1]] + [\"\".join(j) for j in zip(*[word[i:] for i in range(n)])] + [word[-(n-1):] + ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "861103c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' t', 'te', 'es', 'st', 't ']\n"
     ]
    }
   ],
   "source": [
    "print(word2ngrams(\"test\", 2))\n",
    "\n",
    "ngrams = [word2ngrams(word, n=2) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "195413c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbfd850b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17)\t1\n",
      "  (0, 35)\t1\n",
      "  (0, 48)\t1\n",
      "  (0, 61)\t1\n",
      "  (0, 75)\t1\n",
      "  (0, 90)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 52)\t1\n",
      "  (0, 67)\t1\n",
      "  (0, 82)\t1\n",
      "  (1, 17)\t6\n",
      "  (1, 19)\t5\n",
      "  (2, 533)\t1\n",
      "  (2, 639)\t1\n",
      "  (2, 311)\t1\n",
      "  (2, 541)\t1\n",
      "  (2, 589)\t1\n",
      "  (2, 666)\t1\n",
      "  (2, 540)\t1\n",
      "  (2, 646)\t1\n",
      "  (2, 331)\t1\n",
      "  (2, 562)\t1\n",
      "  (2, 609)\t1\n",
      "  (3, 17)\t1\n",
      "  :\t:\n",
      "  (936, 607)\t1\n",
      "  (937, 311)\t1\n",
      "  (937, 589)\t1\n",
      "  (937, 473)\t1\n",
      "  (937, 358)\t1\n",
      "  (937, 388)\t1\n",
      "  (937, 341)\t1\n",
      "  (937, 595)\t1\n",
      "  (937, 365)\t1\n",
      "  (937, 327)\t1\n",
      "  (937, 395)\t1\n",
      "  (937, 355)\t1\n",
      "  (938, 589)\t1\n",
      "  (938, 388)\t1\n",
      "  (938, 610)\t1\n",
      "  (938, 406)\t1\n",
      "  (938, 619)\t1\n",
      "  (939, 541)\t1\n",
      "  (939, 589)\t1\n",
      "  (939, 218)\t2\n",
      "  (939, 191)\t1\n",
      "  (939, 237)\t1\n",
      "  (939, 239)\t1\n",
      "  (939, 592)\t1\n",
      "  (939, 193)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = 'char', lowercase = False, ngram_range = ngram_range)\n",
    "\n",
    "all_words = passwords + words\n",
    "\n",
    "ngram_features = vectorizer.fit_transform(all_words)\n",
    "#print(ngram_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8d54dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(940, 12)\n",
      "<class 'numpy.ndarray'>\n",
      "(9584,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(type(features))\n",
    "print(ngram_features.data.shape)\n",
    "print(type(ngram_features.data))\n",
    "# CountVectorizer returns a sparse matrix. This needs to be converted into a dense matrix in order to be able to concatenate it.\n",
    "features = np.concatenate((np.array(features), ngram_features.toarray()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4e2e3a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# link features and words\n",
    "total = list(zip(all_words, features))\n",
    "#print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed7185",
   "metadata": {},
   "source": [
    "# Machine Learning shizzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fde0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51433a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0401', '101010', 'justin1', 'al', 'marie1', 'of', 'YAgjecc826', 'mickey1', 'stephanie1', 'is', 'green1', 'fuckyou', 'matrix', 'p@ssw0rd', 'onderzoek', 'that', '123654', 'to', 'soccer10', 'qazwsx123', 'in', 'slipknot', '789456', 'love1234', '123456b', 'abcdefg123', 'xbox360', 'family1', 'apple', 'oliver', 'skater1', 'richard', 'Chinese', 'lakers24', 'maverick', 'regeringen', 'or', 'in', 'naruto', 'santiago', 'tend', 'onderzoek', 'mother1', 'dexter', 'that', '$HEX', 'het', 'while', 'gesprek”', 'of', 'which', 'their', 'a', 'of', 'jessie', 'staan', 'mylove', 'george', 'scooter1', 'aaaa', 'that', 'passwords', 'music1', '1', 'soleil', 'blijven', 'destiny', 'poohbear', 'austin1', 'languages', 'zxcvbnm123', 'this', 'orange1', 'lebron23', 'English', 'Chinese', 'do', 'pussy1', 'Li', 'google1', 'naam', 'money1', 'van', 'asdf', '090909', 'find', 'loser1', 'sakura', 'bailey1', 'like', 'than', 'booboo1', 'molly1', '0104', 'booboo', 'mnbvcxz', 'Chinese', 'phoenix', 'sequence', 'wil', 'fucker1', '010101', 'q1w2e3r4', 'more', 'aaaaaaaa', 'example', 'qaz123', 'qwer', 'edward1', 'nicole', 'ze', 'argue', 'Wang', 'include', 'lovers', 'marlboro', 'player', 'likely', 'Li', 'te', 'Staten', 'carlos', 'name', 'abcdef', 'qweqwe', '31', '12121212', 'Finally', 'snel', 'cameron', 'deuren', '123qweasd', 'fuckme1', 'creative', 'Het', 'they', 'danielle1', '3rJs1la7qE', 'bij', 'lovelove', 'greenday', 'voor', 'jonathan1', 'and', 'digits', 'vincent', 'prefer', 'sexy12', 'joshua', 'raiders1', 'qwerty12345', 'is', '50', 'g', 'differences', 'alicia', 'fuckyou2', 'sandra', 'adrian', 'hockey1', 'prefer', 'anoniem', 'De', 'Password', 'barbara', 'spongebob', 'sebastian', 'babygirl1', 'and', 'passwords', 'spiderman', 'het', 'corvette', 'zk.:', 'hoogte', 'found', 'letters', 'football', '888888', 'exacte', 'cutie1', 'sammy1', 'match', 'olivia', 'a123456789', '696969', 'thomas1', 'date']\n"
     ]
    }
   ],
   "source": [
    "X_train_tot, X_test_tot, y_train, y_test = train_test_split(total, labels, test_size=0.2, random_state = 42)\n",
    "X_train = [features for _, features in X_train_tot]\n",
    "words_train = [words for words, _ in X_train_tot]\n",
    "\n",
    "X_test = [features for _, features in X_test_tot]\n",
    "words_test = [words for words, _ in X_test_tot]\n",
    "\n",
    "print(words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00002660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f78293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8776595744680851\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c75358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[73  7]\n",
      " [16 92]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "332badc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dexter\n",
      "1.0\n",
      "\n",
      "$HEX\n",
      "1.0\n",
      "\n",
      "george\n",
      "1.0\n",
      "\n",
      "aaaa\n",
      "1.0\n",
      "\n",
      "1\n",
      "1.0\n",
      "\n",
      "blijven\n",
      "0.0\n",
      "\n",
      "poohbear\n",
      "1.0\n",
      "\n",
      "asdf\n",
      "1.0\n",
      "\n",
      "phoenix\n",
      "1.0\n",
      "\n",
      "sequence\n",
      "0.0\n",
      "\n",
      "qwer\n",
      "1.0\n",
      "\n",
      "Staten\n",
      "0.0\n",
      "\n",
      "qweqwe\n",
      "1.0\n",
      "\n",
      "deuren\n",
      "0.0\n",
      "\n",
      "bij\n",
      "0.0\n",
      "\n",
      "vincent\n",
      "1.0\n",
      "\n",
      "joshua\n",
      "1.0\n",
      "\n",
      "anoniem\n",
      "0.0\n",
      "\n",
      "Password\n",
      "1.0\n",
      "\n",
      "spongebob\n",
      "1.0\n",
      "\n",
      "corvette\n",
      "1.0\n",
      "\n",
      "zk.:\n",
      "1.0\n",
      "\n",
      "hoogte\n",
      "0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_test)):\n",
    "    if y_test[i] != y_pred[i]:\n",
    "        print(words_test[i])\n",
    "        print(y_test[i])\n",
    "        print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyternotebookenvnew",
   "language": "python",
   "name": "jupyternotebookenvnew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
