{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f4e667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-15 12:40:15.027284: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-15 12:40:15.135859: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-07-15 12:40:15.135879: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-07-15 12:40:15.745132: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-15 12:40:15.745193: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-15 12:40:15.745199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from Levenshtein import distance as lev\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d4c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66417da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea4516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67afa464",
   "metadata": {},
   "source": [
    "# Read passwords from file\n",
    "Initially we only read the first 100 passwords\n",
    "\n",
    "If you want to read all the lines, use .readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7746d178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1584b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_passwords(filename, nr_lines):\n",
    "    passwords = []\n",
    "    comparison_pw = []\n",
    "\n",
    "    with open(filename, \"r\") as file:\n",
    "        for i in range(nr_lines):\n",
    "            passwords.append(next(file).strip())\n",
    "            comparison_pw.append(next(file).strip())\n",
    "    return passwords, comparison_pw\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9512c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa44a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_labels(passwords, words):\n",
    "    return np.concatenate((np.ones(len(passwords)), np.zeros(len(words))), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61959c3d",
   "metadata": {},
   "source": [
    "Passwords are 1 and nonpassword strings are 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41976a7a",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "Build features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28699350",
   "metadata": {},
   "source": [
    "## Levenshtein\n",
    "To speed up this calculation, it might be worth it to create an implementation of Levenshtein that stops as soon as a distance larger than the current lowest is found. This increases the average time complexity (but not worst case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "584c6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_levenshtein_distance(word, passwords):\n",
    "    low = 42000\n",
    "    for pw in passwords:\n",
    "        d = lev(word, pw)\n",
    "        if d < low:\n",
    "            low = d\n",
    "    return low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b1b41",
   "metadata": {},
   "source": [
    "## Other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83f5e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_non_repeating(text):\n",
    "    '''\n",
    "    Remove repeating letters from a string\n",
    "    E.g. aaabbbccccccaaa becomes abca\n",
    "    \n",
    "    text: input text\n",
    "    return: text without repeating letters\n",
    "    '''\n",
    "    count = 0\n",
    "    for i, c in enumerate(text):\n",
    "        if i ==0 or c != text[i-1]:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "def counts(word):\n",
    "    alpha_lower = 0\n",
    "    alpha_upper = 0\n",
    "    numeric = 0\n",
    "    special = 0\n",
    "    s = \"\"\n",
    "    for c in word:\n",
    "        if c.islower():\n",
    "            alpha_lower += 1\n",
    "            s += 'L'\n",
    "        elif c.isupper():\n",
    "            alpha_upper += 1\n",
    "            s += 'U'\n",
    "        elif c.isnumeric():\n",
    "            numeric += 1\n",
    "            s += 'N'\n",
    "        else:\n",
    "            special += 1\n",
    "            s += 'S'\n",
    "    length = len(word)\n",
    "    char_sets = bool(alpha_lower) + bool(alpha_upper) + bool(numeric) + bool(special)\n",
    "    lev_d = calculate_levenshtein_distance(word, comparison_pw)\n",
    "    return [length, alpha_lower, alpha_lower/length, alpha_upper, alpha_upper/length, numeric, numeric/length, special, special/length, char_sets, count_non_repeating(s), lev_d]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc284ea",
   "metadata": {},
   "source": [
    "## ngram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8311d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2ngrams(word, n=2):\n",
    "    return [' ' + word[:n-1]] + [\"\".join(j) for j in zip(*[word[i:] for i in range(n)])] + [word[-(n-1):] + ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "861103c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' t', 'te', 'es', 'st', 't ']\n"
     ]
    }
   ],
   "source": [
    "print(word2ngrams(\"test\", 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed7185",
   "metadata": {},
   "source": [
    "# Machine Learning shizzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fde0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "332badc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_errors(y_test, y_pred):\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] != y_pred[i]:\n",
    "            print(words_test[i])\n",
    "            print(y_test[i])\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51433a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ML_model(total, labels, model = \"decisiontree\"):\n",
    "    X_train_tot, X_test_tot, y_train, y_test = train_test_split(total, labels, test_size=0.2, random_state = 42)\n",
    "    X_train = [features for _, features in X_train_tot]\n",
    "    words_train = [words for words, _ in X_train_tot]\n",
    "\n",
    "    X_test = [features for _, features in X_test_tot]\n",
    "    words_test = [words for words, _ in X_test_tot]\n",
    "    \n",
    "    \n",
    "    # pick model\n",
    "    if model == \"decisiontree\":\n",
    "        clf = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy score: {}\".format(accuracy_score(y_pred, y_test)))\n",
    "\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    #print(words_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aeb81e",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "447d45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f10de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "lr = 0.01\n",
    "n_folds = 5\n",
    "lstm_input_size = 32\n",
    "hidden_state_size = 256\n",
    "n_layers = 2\n",
    "dropout = 0.125\n",
    "bidirectional = True\n",
    "batch_size = 30\n",
    "num_sequence_layers = 2\n",
    "output_dim = 11                       # !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "num_time_steps = 4000                 # !!!!!!!!!!!!!!\n",
    "rnn_type = 'LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d5e2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=11, num_layers=2, rnn_type='LSTM'):\n",
    "        super(Bi_RNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #Define the initial linear hidden layer\n",
    "        self.init_linear = nn.Linear(self.input_dim, self.input_dim)\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = eval('nn.' + rnn_type)(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear1 = nn.Linear(512,128)\n",
    "        self.linear2 = nn.Linear(128,8)\n",
    "        self.linear3 = nn.Linear(8,2)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Creating PackedSequence\n",
    "        #packing = nn.utils.rnn.pad_sequence(input)\n",
    "        #                                                  !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # Insert packing stuff\n",
    "        \n",
    "        \n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [batch_size, input_size ,hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both\n",
    "        # have shape (batch_size, num_layers, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(linear_input)\n",
    "        \n",
    "        \n",
    "        # Unpacking PackedSequence\n",
    "        #                                                  !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # Insert packing stuff\n",
    "        \n",
    "        \n",
    "        # Something with Permute???????                    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        \n",
    "        # Tanh\n",
    "        nn.Tanh(input)\n",
    "        \n",
    "        # MaxPool 1D\n",
    "        nn.MaxPool1d()\n",
    "        \n",
    "        # Tanh\n",
    "        nn.Tanh(input)\n",
    "        \n",
    "        # Squeeze\n",
    "        \n",
    "        # First linear layer\n",
    "        input = self.linear1(input)\n",
    "        \n",
    "        # Dropout\n",
    "        \n",
    "        # Second linear layer\n",
    "        input = self.linear2(input)\n",
    "        \n",
    "        # Dropout\n",
    "        \n",
    "        # Third linear layer\n",
    "        output = self.linear3(input)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1224e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Sequential(Dataset):\n",
    "    def __init__(self, input, output):\n",
    "        self.embedding = nn.Embedding(100,32)\n",
    "        self.input = self.embedding(input)\n",
    "        self.output = output\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input[idx]\n",
    "        y = self.output[idx]\n",
    "        # Embed the input in numbers rather than characters\n",
    "        x = self.embedding(x)\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "        return x, y\n",
    "\n",
    "class Dataset_Sequential_test(Dataset):\n",
    "    def __init__(self, input):\n",
    "        self.embedding = nn.Embedding(100,32)\n",
    "        self.input = self.embedding(input)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input[idx]\n",
    "        # Embed the input in numbers rather than characters\n",
    "        x = self.embedding(x)\n",
    "        x = torch.tensor(x, dtype=torch.str)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a8c86c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m Dataset_Sequential_test(X_test)\n\u001b[1;32m      2\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "test = Dataset_Sequential_test(X_test)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.MaxPool1d(3, stride = 2)\n",
    "input = torch.randn(3, 4, 5)\n",
    "output = m(input)\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LSTM():\n",
    "    #Iterate through folds\n",
    "\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    local_val_score = 0\n",
    "    models = {}\n",
    "\n",
    "    k=0 #initialize fold number\n",
    "    for tr_idx, val_idx in kfold.split(words_train, y_train):\n",
    "\n",
    "        print('starting fold', k)\n",
    "        k += 1\n",
    "\n",
    "        print(6*'#', 'splitting and reshaping the data')\n",
    "        words_train = np.array(words_train).flatten()\n",
    "        train_input = words_train[tr_idx]\n",
    "        train_target = y_train[tr_idx]\n",
    "        val_input = words_train[val_idx]\n",
    "        val_target = y_train[val_idx]\n",
    "\n",
    "        print(6*'#', 'Loading')\n",
    "        train = Dataset_Sequential(train_input, train_target)\n",
    "        valid = Dataset_Sequential(val_input, val_target)\n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        #Build tensor data for torch\n",
    "        train_preds = np.zeros((int(train_input.shape[0] * 2)))\n",
    "        val_preds = np.zeros((int(val_input.shape[0] * 2)))\n",
    "        best_val_preds = np.zeros((int(val_input.shape[0] * 2)))\n",
    "        train_targets = np.zeros((int(train_input.shape[0] * 2)))\n",
    "        avg_losses_f = []\n",
    "        avg_val_losses_f = []\n",
    "\n",
    "        #Define loss function\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        #Build model, initialize weights and define optimizer\n",
    "        model = Bi_RNN(lstm_input_size, hidden_state_size, batch_size=batch_size, output_dim=output_dim, num_layers=num_sequence_layers, rnn_type=rnn_type)  # (input_dim, hidden_state_size, batch_size, output_dim, num_seq_layers, rnn_type)\n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)  # Using Adam optimizer\n",
    "        #scheduler = ReduceLROnPlateau(optimizer, 'min', patience=150, factor=0.1, min_lr=1e-8)  # Using ReduceLROnPlateau schedule\n",
    "        temp_val_loss = 9999999999\n",
    "        reached_val_score = 0\n",
    "\n",
    "        #Iterate through epochs\n",
    "        for epoch in range(n_epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            #Train\n",
    "            model.train()\n",
    "            avg_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "                x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n",
    "                y_batch = y_batch.view(-1, num_time_steps, output_dim)\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_batch.cuda())\n",
    "                loss = loss_fn(y_pred.cpu(), y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "                pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "                train_preds[i * batch_size * train_input.shape[1]:(i + 1) * batch_size * train_input.shape[1]] = pred.reshape((-1))\n",
    "                train_targets[i * batch_size * train_input.shape[1]:(i + 1) * batch_size * train_input.shape[1]] = y_batch.detach().cpu().numpy().argmax(axis=2).reshape((-1))\n",
    "                del y_pred, loss, x_batch, y_batch, pred\n",
    "\n",
    "            #Evaluate\n",
    "            model.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n",
    "                y_batch = y_batch.view(-1, num_time_steps, output_dim)\n",
    "                y_pred = model(x_batch.cuda()).detach()\n",
    "                avg_val_loss += loss_fn(y_pred.cpu(), y_batch).item() / len(valid_loader)\n",
    "                pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n",
    "                val_preds[i * batch_size * val_input.shape[1]:(i + 1) * batch_size * val_input.shape[1]] = pred.reshape((-1))\n",
    "                del y_pred, x_batch, y_batch, pred\n",
    "            if avg_val_loss < temp_val_loss:\n",
    "                temp_val_loss = avg_val_loss\n",
    "\n",
    "            #Calculate F1-score\n",
    "            train_score = f1_score(train_targets, train_preds, average='macro')\n",
    "            val_score = f1_score(val_target.argmax(axis=2).reshape((-1)), val_preds, average='macro')\n",
    "\n",
    "            #Print output of epoch\n",
    "            elapsed_time = time.time() - start_time\n",
    "            scheduler.step(avg_val_loss)\n",
    "            if epoch%10 == 0:\n",
    "                print('Epoch {}/{} \\t loss={:.4f} \\t train_f1={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} \\t time={:.2f}s'.format(epoch + 1, n_epochs, avg_loss, train_score, avg_val_loss, val_score, elapsed_time))\n",
    "\n",
    "            if val_score > reached_val_score:\n",
    "                reached_val_score = val_score\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "                best_val_preds = copy.deepcopy(val_preds)\n",
    "\n",
    "        #Calculate F1-score of the fold\n",
    "        val_score_fold = f1_score(val_target.argmax(axis=2).reshape((-1)), best_val_preds, average='macro')\n",
    "\n",
    "        #Save the fold's model in a dictionary\n",
    "        models[k] = best_model\n",
    "\n",
    "        #Print F1-score of the fold\n",
    "        print(\"BEST VALIDATION SCORE (F1): \", val_score_fold)\n",
    "        local_val_score += (1/n_folds) * val_score_fold\n",
    "\n",
    "    #Print final average k-fold CV F1-score\n",
    "    print(\"Final Score \", local_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7905f06",
   "metadata": {},
   "source": [
    "# Run experiments\n",
    "The following code allows you to run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca5649",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_filename = \"darkweb2017-top10000.txt\"\n",
    "nr_lines = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "passwords, comparison_pw = read_passwords(pw_filename, nr_lines)\n",
    "#words = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf1b4c",
   "metadata": {},
   "source": [
    "## Feature-based models\n",
    "\n",
    "We have the following feature settings\n",
    "\n",
    "- base_features: simple feature set such as the length of the word and amount of characters from different character sets\n",
    "- levenshtein: calculate the Levensthein distance to most common passwords\n",
    "- ngrams: ngram of characters as features\n",
    "- ngram_range: if ngrams is true, this setting determines which ngrams are to be taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dcefbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = True\n",
    "levenshtein = True\n",
    "ngrams = True\n",
    "ngram_range = (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec45f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19278a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run feature-based models\n",
    "labels = create_feature_labels(passwords, words)\n",
    "# replace next line as soon as we have code that reads in the text file\n",
    "text = \"Previous research has shown that language and culture influence what passwords look like. Li et al. (2014) compared Chinese and English passwords, and found several differences. First of all, Chinese speakers prefer to use digits while English speakers prefer letters, especially lowercase letters; around 50% of the Chinese passwords were digits only. Besides that, all users use patterns that they are familiar with. In the case of Chinese speakers, this is pinyin words, while in the case of English speakers, these are English words. Finally, they also find that users tend to use date formats that are customary in their culture (e.g. 0104 or 0401 for April 1st). Wang et al. (2019), who also compared Chinese and English passwords, add to this that there are difference in the letter distributions of password groups. For example, Chinese passwords use the letter q more often than English passwords do, likely because q is a common letter in pinyin, but quite rare in English. These distributions do not exactly match the distributions of the languages completely. For example, they found that the letters l and w occurred more often in passwords than in general Chinese pinyin. They argue that this is likely because they are part of the very popular Chinese names Li and Wang. Furthermore, Wang et al. (2019) add that around 11% of Chinese passwords contain a pinyin name (compared to 4% in their English passwords dataset) and about 31% include a 4+ digit date in their password, of which 17% include a 6 digit date. Note that these percentages are likely overestimations as it is not always possible to determine of a number sequence whether it is a date or refers to something else. All these numbers are significantly higher than in the English password set. Phone numbers are also possible passwords. De Tweede Kamerleden waren het er snel over eens: als ze onderzoek wilden doen naar beïnvloeding van moskeeën, dan móésten ze Lorenzo Vidino wel uitnodigen. De gelauwerde extremisme-deskundige uit de Verenigde Staten had er boeken over geschreven, parlementen voorgelicht, regeringen geadviseerd. Dus leek het een logische keuze voor de Kamercommissie die in 2019 onderzoek deed naar buitenlandse moskeefinanciering, om Vidino te horen als expert. Het werd een „heel interessant gesprek”, herinnert een van de commissieleden zich, die anoniem wil blijven omdat de zitting achter gesloten deuren plaatsvond. „Hij schetste een gedetailleerd beeld van de Nederlandse vertakkingen van de Moslimbroederschap.” De islamitische beweging zou hier volgens Vidino „verdeeldheid” zaaien. Hij kon meerdere Nederlandse moskeeën bij naam noemen die onder invloed zouden staan van de Moslimbroeders en wist zelfs de exacte hoogte van ontvangen giften uit Qatar.\"\n",
    "words = text_to_word_sequence(text, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2816b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [counts(pw) for pw in passwords]\n",
    "features_word = [counts(word) for word in words]\n",
    "features = np.concatenate((features, features_word), axis=0)\n",
    "\n",
    "#ngrams = [word2ngrams(word, n=2) for word in words]\n",
    "vectorizer = CountVectorizer(analyzer = 'char', lowercase = False, ngram_range = ngram_range)\n",
    "\n",
    "all_words = passwords + words\n",
    "\n",
    "ngram_features = vectorizer.fit_transform(all_words)\n",
    "\n",
    "\n",
    "# CountVectorizer returns a sparse matrix. This needs to be converted into a dense matrix in order to be able to concatenate it.\n",
    "features = np.concatenate((np.array(features), ngram_features.toarray()), axis=1)\n",
    "\n",
    "# link features and words\n",
    "total = list(zip(all_words, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d20bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run neural network model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyternotebookenvnew",
   "language": "python",
   "name": "jupyternotebookenvnew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
